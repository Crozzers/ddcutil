Several weeks ago I posted a message about the desirability of using DDC to record and reset monitor settings as part of profiling.  The gist of the responses was that DDC was poorly or incompletely implmented by monitor manufacturers.   I decided to try to come up with a solution that at least worked for MY set of monitors.   After much pain, I can now (with qualification, see below) adjust color related monitor settings from the computer, record the color related monitor settings before profiling, and restore the corresponding settings when loading a profile.  

*) /dev/i2c-n permissions

UDEV rule 

2) require i2c_dev module  

modprobe.d entry 

*) i2c-tools 

*) works with nouveau and radeaon open source drivers 

fails with nvidia driver 

fglrx doesn't even report /dev/i2c-n 

*) DDC doc specified in terms of bytes on the wire.  Understanding how smbus calls map to calls in terms of DDC protocol not clear.   Summary:  i2c_smbus_...   do the right thing.   Simply calling read and write on the open /dev/ic2-n file handle also seem to work.  


Problem with i2c.. functions:  data size is 32 bytes max. 
capabilities fragment sent from monitor can have fragment size up to 32 bytes, plus 5? bytes additional data, which is 37 bytes of data for i2c.. to process.   Truncated.


i2c_smbus_read_i2c_block_data() 
i2c_smbus_write_i2c_block_data() 






*) One monitor too many errors to reliably read capabilities. 

*) Some variation, but that's the least of the problems. 

- return Null Response vs setting Unsupported Feature bit. 

- capabilities string usually specified as sequence of hex characters "(01 05 0c)", but in one case returned as what appears to be decimal numbers "(1 3)".  


*) Only using VCP features defined in the standard.   No attempt to use manufacturer specific extensions.


Kernel module i2c_algo_bit is not present when the nvidia driver is running.  "lsmod | grep i2c" reports nothing.  FALSE

WRONG:   can SuSE: can modprobe i2c_piix4, which loads i2c_dev as well
can also modprobe i2c_algo_bit   - now write failes with errno=5 EIO

suse: i2c_core exists only for xen kernel


README.txt 

Appendix F 

old - refers to 2.4 and 2.6 kernels 
says i2c-core kernel module required

The following functionality is currently supported:

I2C_FUNC_I2C
I2C_FUNC_SMBUS_QUICK
I2C_FUNC_SMBUS_BYTE
I2C_FUNC_SMBUS_BYTE_DATA
I2C_FUNC_SMBUS_WORD_DATA





Post for LM-Sensors list

I've been trying to use i2c-dev to read and control monitor settings, using the DDC/CI protocol to communicate with address 0x37 on device /dev/i2c-n.  Things are sort of working (details below),  But it's not clear to me to what extent I'm hitting real limits, or if I just don't know what I'm doing.  Perhaps I shouldn't even be trying to use i2c-dev for this application.  Advice appreciated.  And if there's a more appropriate place to post this question, I'd appreciate hearing that as well.

To review: A monitor is accessed via device /dev/i2c-n, created by the video device driver.  EDID data is found by reading address 0x50.  The monitor's settings are read and written at address 0x37.

I can communicate with the monitor if either the open-source nouveau or radeon drivers are loaded.  Both support i2c_smbus_read_i2c_block_data() and i2c_smbus_write_i2c_block_data(), which put the right bytes on the wire and handle the ack and nack bits as per the DDC specification.  (See documentation file i2c/smbus-protocol.)

For the nouveau driver running on Fedora 20, "lsmod | grep i2c" reports: 

i2c_piix4              22155  0 
i2c_algo_bit           13257  1 nouveau
i2c_dev                14027  0 
i2c_core               38656  6 drm,i2c_dev,i2c_piix4,drm_kms_helper,i2c_algo_bit,nouveau


Here's a simplified example (minimal error checking) of how I'm reading the monitor's brightness setting: 

   int fh = open("/dev/i2c-0",  O_NONBLOCK|O_RDWR);
   ioctl(fh, I2C_SLAVE, 0x37);

   unsigned char zeroByte = 0x00;
   write(fh, &zeroByte, 1);   // seems to be necessary to reset monitor state

   unsigned char ddc_cmd_bytes[] = { 
      0x6e,              // address 0x37, shifted left 1 bit 
      0x51,              // source address 
      0x02 | 0x80,       // number of DDC data bytes, with high bit set
      0x01,              // DDC Get Feature Command 
      0x10,              // Feature, Luminosity
      0x00,              // checksum, to be set
   };
   ddc_cmd_bytes[5] = ddc_checksum(ddc_cmd_bytes, 5);    // calculate DDC checksum on all bytes 
   i2c_smbus_write_i2c_block_data(fh, ddc_cmd_bytes[1], sizeof(ddc_cmd_bytes)-2, ddc_cmd_bytes+2);
   // alt: write(fh, ddc_cmd_bytes+1, sizeof(ddc_cmd_bytes)-1);     // see below
   usleep(5000); 

   unsigned char ddc_response_bytes[12];
   unsigned char cmd_byte = 0x00;   // apparently ignored, can be anything
   i2c_smbus_read_i2c_block_data(fh, cmd_byte, 11, readbuf+1);
   // alt read(fh, readbuf+1, 11);   // see below
   ddc_response_bytes[0] = 0x50;     // for proper checksum calculation
   int calculated_checksum = ddc_checksum(readbuf, 11); 
   assert(readbuf[11] == calculated_checksum); 
   int response_len = ddc_response_bytes[2] & 0x7f;       // always 8 for DDC Get Value response
   // now parse the response data


When issuing the DDC get feature command (code 0x01), a fixed data block of 12 data bytes is returned as shown above (as counted from the i2c_cmsbus_read_i2c_block_data() perspective. However, the DDC Get Capabilities request (0xf3), can return up to 39 bytes (fixed DDC data of 7 bytes plus a "fragment" of up to 32 bytes, depending on the monitor being communicated with.).   This is greater than the 32 byte max data size supported by i2c_smbus_read_i2c_block_data() (constant I2C_SMBUS_I2C_BLOCK_MAX in i2c-dev.h).  And indeed, I've seen i2c_smbus_read_i2c_block_data() return truncated responses.  

Now things get interesting.  

Simply using write() and read() seems to work, when the i2c_smbus_..._i2c_block_data() calls in the above code are replaced by the commented out write() and read() lines.  So apparently apparently write() and read() are handling the algorithm bits (start, top, ack, nack) properly.


Now come the key questions: 

Am I just getting lucky here, or is the i2c_dev driver (or one of the drivers it calls) really doing the right thing for managing the algorithm bits for the i2c DDC protocol? 

Is there a better way to use lower level services in i2c-dev? 

Should I really be writing a device driver? 


Finally, the proprietary video drivers. 

The proprietary nvidia driver creates the /dev/i2c-n devices.  I can read the monitor EDID information on bus address 0x50. and get the functionality flags using ioctl I2C_FUNCS.   Functions ic2_smbus_read_i2c_block_data() and is2_smbus_write_i2c_block_data() are not supported (flags I2C_FUNC_SMBUS_READ_I2C_BLOCK and I2C_FUNC_SMBUS_WRITE_I2C_BLOCK are not set).  Attempting to call these functions fails with errno=22 (EINVAL - invalid argument) if module i2c_algo_bit has not been loaded, and errno=5 (EIO - IO Error) if it has.

Trying to use write() and read() as described above also fails.  write() appears to succeed, but read() returns invalid data.

Appendix F of the Nvidia driver README discusses i2c support.  It appears to be quite dated, referring to the 2.4 and 2.6 kernels.  According to this document, only the following functionality is supported, which would be consistent with what I've seen:  I2C_FUNC_I2C, I2C_FUNC_SMBUS_QUICK, I2C_FUNC_SMBUS_BYTE, I2C_FUNC_SMBUS_BYTE_DATA, I2C_FUNC_SMBUS_WORD_DATA


As for the proprietary fglrx driver, it doesn't even create the /dev/i2c-n devices.   End of story. 


====  Snippets for reply to Delvare on linux-i2c ====

Jean, 

Thanks for your detailed reply.  

Let me first elaborate on the DDC/CI protocol, which will probably be clearer than splitting the information up into snippets on individual comments.   

The DDC/CI protocol is defined in VESA document DDC/CI Standard 1.1, file name ddcciv1r1.pdf.  An earier version 1.0, file name ddcciv1.pdf, is less clear, but it does have a number of comments that clarify behavior of the protocol.  The actual commands that are transmitted over DDC/CI are detailed in the VESA Monitor Control Command Set (MCCS) document, file name mccsV3.pdf.  These files can be a bit hard to locate, but googling the file names finds them readily. 

The I2C bus notation used in the DDC/CI specification is as follows: 

S       Start bit             Generated by the master to start communication (bus becomes busy)
XX      Data byte, hex        Made of 8 data bits, may be sent or received by the master
a       Acknowledge bit       Sent in the opposite direction of the data bits
n       Non acknowledge bit   Signals the end of the data transfer, a stop bit should follow
                              to free the bus.
P       Stop bit              Signals fhte end of the communication, bus becomes free
CHK     Checksum              XOR of preceding bytes
CHK'    Checksum              Checksum computed using the 0x50h virtual address
VCP     VCP code              Virtual Control Panel feature code (1 byte, e.g. 0x10 for Luminosity)

DDC/CI Get VCP Feature and VCP Feature Reply is defined as follows.

The following sequence is written by the host:

S-6Ea-51a-82a-01a-CPa-CHKa-P

Where: 
   6E     Destination address
   51     Source address
   82     number of data bytes following (not including checksum) XOR 0x80
   01     Get VCP Feature opcode
   CP     VCP Feature code (10h for luminosity)

The display responds with:

S-6Fa-6Ea-88a-02a-RCa-CPa-TPa-MHa-MLa-SHa-SLa-CHK'n-P

Where: 
   6F    Destination address
   6E    Source address
   88    number of data bytes following (not including checksum) XOR 0x80
   02    VCP Feature reply opcode
   RC    Result code, 00=No Error
   CP    VCP Feature code from Feature Request message (10h for luminosity)
   TP    VCP type code (will be 00h for Luminosity)
   MH    Max value high byte
   ML    Max value low byte
   SH    Present value high byte
   SL    Present Value low byte


===

No good reason.  O_NONBLOCK is the result of copy and paste from some blog post when I was first trying to get things to work.  I've eliminated the flag, and all my code works unchanged.



=== re 11 bytes, not 12

Correct.  I skipped over things.   I had to stand on my head three times over and look sideways to properly munge 
the DDC spec into the i2c-dev calls.  From the DDC perspective, 1 more byte is written and read than are written and read using write(), read(), i2c_smbus_write_i2c_block_data(), etc.  Byte 0 of a DDC request or response is implicit (the result of ioctl(fh, I2C_SLAVE, 0x37);) but from a DDC perspective you need to think of it as being there for the checksums to be properly calculated.

=== re direction changes

As described in the DDC spec above, a DDC exchange is a write of several bytes, followed by a read of several bytes.  There's no direction change within the read. 


== re spec location 

See above.


=== why doing this

Some context may be appropriate here.  This project came about because of my interest in monitor profiling and calibration.  A monitor calibration is relative to the current monitor settings for luminosity, contrast, color temperature, etc.  Setting a profile only makes sense if you know that the monitor settings are unchanged from when you made the profile.  Currently in Linux, the only way to do this is leave the buttons on the monitor untouched.   None of the major profiling tools - dispcal, dispcalGUI, colord, oyranos - record or restore the monitor settingss.  At most they can give you advice on how to adjust the physical monitor controls to obtain the desired color temperature.  There once was a program to manage monitor settings, ddccontrol, but it hasn't been maintained and has been dropped from distros.  When I inquired about this on the color lists, the gist of the responses was that it was too hard to do because the DDC spec was inconsisently implemented across monitors.  I decided to try to come up with a solution that works for at least MY set of monitors.  Basically, at the time of monitor calibration, the program records whatever color related settings exist for the monitor, and can then restore those settings when the profile is set.   Interestingly, the variation amoung monitor implementations has proven quite manageable.  Almost all the effort has gone into getting DDC over I2C right.  At this point, it appears that the biggest obstacle to a general solution comes from the proprietary drivers. 

=== 

i2cdetect -F 0 reports: 

Functionalities implemented by /dev/i2c-0:
I2C                              yes
SMBus Quick Command              yes
SMBus Send Byte                  yes
SMBus Receive Byte               yes
SMBus Write Byte                 yes
SMBus Read Byte                  yes
SMBus Write Word                 yes
SMBus Read Word                  yes
SMBus Process Call               no
SMBus Block Write                no
SMBus Block Read                 no
SMBus Block Process Call         no
SMBus PEC                        no
I2C Block Write                  no
I2C Block Read                   no


This is (unsurprisingly) consistent with what I get calling ioctl I2C_FUNCS.

===

I'll rework my code to use ioctl I2C_RDWR and report back if by some miracle it works.

As a first test case, I just replaced the write() call with ioctl I2C_RWWR.  It works for nouveau, and fails with errno=5 (EIO) for the proprietary nvidia driver.  


===================================================================================

==== Nvidia posting ====

Title: Unable to write to I2C address 0x37, GTX660Ti

I'm trying to communicate with monitors using DDC/CI over I2C on /dev/i2c-n address 0x37. 

I can read monitor EDID data at address 0x50.  I can also read address 0x37 to detect that the address is live.  However, writes to address 0x37 fail on my GTX660Ti, so DDC/CI fails.

I've reduced the failure situation to the following example, which shows writes of varying sizes.  On the chance that the content being written might affect the result, the code attempts to write two different sequences of bytes: a valid DDC Feature Request, and a sequence of 0x00 bytes.   All test cases succeed or the open source nouveau and radeon drivers.  They also all succeed on a system with a GT210 using driver 331.38.  

For a GT660ti with either the 331.38 or 337.12 drivers, writes succeed if 1 or 2 bytes are written.  Writes of 3 or more bytes fail with errno=5 (EIO).









Note: if the write() calls are replaced with ioctl I2C_RDWR calls, the same situation holds: success with the open source nouveau and radeon drivers, success with GT210 and nvidia driver 331.38, error EIO with GTX660Ti and nvidia drivers 331.38 or 337.12. 

Tested environments:

GTX660Ti, OpenSUSE 13.1, nvidia 337.12      (checked) 
GTX660TI, Ubuntu 14.04, nvidia driver 331.38 (checked)

works:
GT610, Mint 17, nvidia driver 331.38

Is there a workaround for this situation, an alternative method to execute DDC/CI on the GTX660Ti, or is this simply a failure of the proprietary nvidia driver on newer cards? 


as posted: 


I'm trying to communicate with monitors using DDC/CI over I2C on /dev/i2c-n address 0x37. 

I can read monitor EDID data at address 0x50.  I can also read address 0x37 to detect that the address is live.  However, writes to address 0x37 fail on my GTX660Ti, so DDC/CI fails.

I've reduced the failure situation to the following example, which shows writes of varying sizes.  On the chance that the content being written might affect the result, the code attemtps to write two different sequences of bytes: a valid DDC Feature Request, and a sequence of 0x00 bytes.   All test cases succeed or the open source nouveau and radeon drivers.  They also all succeed on a system with a GT210 using driver 331.38.  

For a GT660ti with either the 331.38 or 337.12 drivers, writes succeed if 1 or 2 bytes are written.  Writes of 3 or more bytes fail with errno=5 (EIO).

Here's the code:
[code]
void demo_nvidia_bug_sample_code(int busno) {
   printf("\n(%s) Starting   \n", __func__ );
   char * writefunc = "write";
   int rc;
   char devname[12];
   snprintf(devname, 11, "/dev/i2c-%d", busno);

   int fh = open(devname,   O_NONBLOCK|O_RDWR);
   ioctl(fh, I2C_SLAVE, 0x37);

   // try a read, it succeeds
   unsigned char * readbuf = calloc(sizeof(unsigned char), 256);
   rc = read(fh, readbuf+1, 1);
   if (rc < 0) {
      printf("(%s) read() returned %d, errno=%s. Terminating execution  \n", __func__, rc, errno_name(errno) );
      exit(1);
   }
   printf("(%s) read succeeded.  Address 0x37 active on %s\n", __func__, devname);

   unsigned char zeroBytes[5] = {0};  // 0x00;

   unsigned char ddc_cmd_bytes[] = {
      0x6e,              // address 0x37, shifted left 1 bit
      0x51,              // source address
      0x02 | 0x80,       // number of DDC data bytes, with high bit set
      0x01,              // DDC Get Feature Command
      0x10,              // Luminosity feature code
      0x00,              // checksum, to be set
   };
   ddc_cmd_bytes[5] = ddc_checksum(ddc_cmd_bytes, 5, false);    // calculate DDC checksum on all bytes
   assert(ddc_cmd_bytes[5] == 0xac);

   printf("\n(%s) Try writing fragments of DDC request string...\n", __func__ );
   int bytect;
   for (bytect=sizeof(ddc_cmd_bytes)-1; bytect > 0; bytect--) {
      usleep(5000);
      errno = 0;
      rc = write(fh, ddc_cmd_bytes+1, bytect);
      if (rc == bytect)
         printf("(%s) bytect=%d, %s() returned rc=%d as expected\n", __func__, bytect, writefunc, rc);
      else if (rc < 0)
         printf("(%s) bytect=%d, Error. %s(), returned %d, errno=%s\n", __func__, bytect, writefunc, rc, errno_name(errno));
      else
         printf("(%s) bytect=%d, Truly weird. rc=%d\n", __func__, bytect, rc);
   }

   printf("\n(%s) Try writing null bytes...\n", __func__ );
   for (bytect=sizeof(zeroBytes); bytect > 0; bytect--) {
      usleep(5000);
      errno = 0;
      rc = write(fh, zeroBytes, bytect);
      if (rc == bytect)
         printf("(%s) bytect=%d, %s() returned rc=%d as expected\n", __func__, bytect, writefunc, rc);
      else if (rc < 0)
         printf("(%s) bytect=%d, Error. %s(), returned %d, errno=%s\n", __func__, bytect, writefunc, rc, errno_name(errno));
      else
         printf("(%s) bytect=%d, Truly weird. rc=%d\n", __func__, bytect, rc);
   }
   close(fh);

} 
[/code]

And here's the output when executed with a GTX660Ti and driver 331.38:

[code]
(demo_nvidia_bug_sample_code) Starting   
(demo_nvidia_bug_sample_code) read succeeded.  Address 0x37 active on /dev/i2c-0

(demo_nvidia_bug_sample_code) Try writing fragments of DDC request string...
(demo_nvidia_bug_sample_code) bytect=5, Error. write(), returned -1, errno=5 - EIO      ( I/O error )
(demo_nvidia_bug_sample_code) bytect=4, Error. write(), returned -1, errno=5 - EIO      ( I/O error )
(demo_nvidia_bug_sample_code) bytect=3, Error. write(), returned -1, errno=5 - EIO      ( I/O error )
(demo_nvidia_bug_sample_code) bytect=2, write() returned rc=2 as expected
(demo_nvidia_bug_sample_code) bytect=1, write() returned rc=1 as expected

(demo_nvidia_bug_sample_code) Try writing null bytes...
(demo_nvidia_bug_sample_code) bytect=5, Error. write(), returned -1, errno=5 - EIO      ( I/O error )
(demo_nvidia_bug_sample_code) bytect=4, Error. write(), returned -1, errno=5 - EIO      ( I/O error )
(demo_nvidia_bug_sample_code) bytect=3, Error. write(), returned -1, errno=5 - EIO      ( I/O error )
(demo_nvidia_bug_sample_code) bytect=2, write() returned rc=2 as expected
(demo_nvidia_bug_sample_code) bytect=1, write() returned rc=1 as expected

[/code]

Environments in which the test case shows errors:

GTX660Ti, OpenSUSE 13.1, nvidia driver 337.12
GTX660Ti, Ubuntu 14.04, nvidia driver 331.38

Environments in which the test case shows no errors (and DDC/CI works):
GTX660Ti, Fedora 20, nouveau open source driver
AMD card, OpenSuse 13.1, readeon open source driver
GT210, Mint 17, nouveau driver
GT210, Mint 17, nvidia driver 331.38

If I can come up with additional Nvidia cards, I will test them in the testbed Mint 17 system and post the results.

Is there a workaround for this situation, an alternative method to execute DDC/CI on the GTX660Ti, or is this simply a failure of the proprietary nvidia driver on newer cards? 

=====

Forgive me for jumping in.   I'm a noob at I2C.   But I have built a couple substantial error injection frameworks over the years, one for a mainframe DBMS and one for Java's checked exceptions.   So while I have nothing useful to say about how to inject exceptions here, I have thought a lot about use cases.

Failing all the time is the necessary first step.  It allows for testing error paths without manually inserting a failure and recompiling, and makes it possible to build unit tests.

Failing randomly (or pseudo-randomly) is important for testing the overall recovery mechanism, particularly where you have an inherently unreliable subsystem like networks or I2C.   By changing the failure rate you can explore, for example, at what point the failure rate of the lower level system becomes so great that it makes the upper level system unreliable.

The one use case I would add, and it may be outside the scope here, is data errors.  The DDC Get Capabilities request entails multiple write/read exchanges, with responses of up to 37 bytes each.   Most of the time this works ok, but I have one monitor that produces a high volume of data errors (double bytes or missing bytes).   This is only detected by examining the data itself (fixed fields and checksum). 

Sanford


=====

reply to Delvare 

First order answer:  the calibration process creates a lookup table that is loaded into the graphics card.  The lookup table translates the color input values into output values that will precisely show the expected color on the screen. 

Second order answer:  What is loosely referred to as "profiling" consists of two pieces, "calibration" and "characterization".  Calibration is the process of bringing a monitor into a precisely known state.   That is the function of the lookup table.   Characterization is process of describing a monitor's capabilities, e.g. what colors it is capable of displaying.  The characerization tables are used by color managed applications, for example, to appropriately render an image whose colors are beyond the range of what the monitor can show.   Both pieces of information are stored in the profile file.  

Very high end monitors, such as the HP Dreamccolor and various EIZOs  expose a LUT that can be used for color translation.  While the DDC protocol allows for manipulating the monitor LUT, some of these monitors instead use a special connection such as USB and proprietary protocol.  Very high end LCD monitors will also have separate red, green, and blue LEDs or CCFL tubes, so that the color temperature can be prceisely adjusted.  But most flat panel monitors  have just one one type of "white" LED or CCFL tube.  

Linux calibration programs, such as Argyllcms, simply take as given whatever state the on screen controls have put the monitor into, and build the lookup table for that state.   dispcalGUI has an optional pre-calibration step which displays the measured color temperature of the display and suggests which phyical monitor controls the user should change to achieve the desired color temperature.

There's a school of throught that holds that unless you have a high end monitor, you're actually best off setting all the controls (excepting brightness) on a LCD monitor to the factory defaults.  Graeme Gill, create of Argyllcms, has a well considered discussion of this question:
http://www.arbyllcms/monitor-controls.html.

----

I was able to narrow down the nvidia driver problem to just the write() call, so the delay plays no role.  Moreover, I only see the problem on my (relatively) new GTX660Ti.   write() calls of 1 or 2 bytes succeed.   Calling write() for 3 or more bytes fails.   The problem does not occur on an older GT220 card.  So it appears that the driver was not properly upgraded to handle newer cards.   I posted a report along with sample code on the Nvidia developer website ( https://devtalk.nvidia.com/default/topic/760798/linux/ddc-ci-over-i2c-fails-gtx660ti/ ).  Other than a comment from one other user who believed that this was the same issue as the known problem of ddccontrol not working for more recent cards, there's been no response.

===

to write: fglrx 
has API calls for reading EDID, wiring and reading DDC 
no need to program retry (apparently) 
secret sauce

to write: use tracing - one particular monitor duuble bytes

also: Wayland   - Hughes thread 
DDC belondgs in X11 or Wayland


As posted: 

Hi John,

Well, now it's my turn to be late in replying.    There's been progress.

On 07/28/2014 03:52 AM, Jean Delvare wrote:
> Hi Sanford,
>
> Sorry for the late reply and thanks for the detailed explanation.
>
>
>
> BTW your test program is only waiting for 5 ms instead of the 40 ms
> suggested in the specification. Could this explain your problems?
Just sloppiness in preparing the simplified demo code.  I'm using 50 ms in the actual code whenever the spec calls for 40 ms, and I'm generous in adding delays for retrys.  The problem with the nvidia driver is unrelated to delays (see below).
> Pardon my ignorance but if color profile managers don't currently set monitor settings using DDC/CI, then what are they doing? Are they only doing software color correction? Or asking the graphics card to do so? 
Basic answer:  the calibration process creates a lookup table that is loaded into the graphics card.  The lookup table translates the color input values into output values that will precisely show the expected color on the screen.

Long winded answer:  What is loosely referred to as "profiling" consists of two pieces, "calibration" and "characterization".  Calibration is the process of bringing a monitor into a precisely known state.   That is the function of the lookup table.   Characterization is process of describing a monitor's capabilities, e.g. what colors it is capable of displaying.  The characterization tables are used by color managed applications, for example, to appropriately render an image whose colors are beyond the range of what the monitor can show.   Both pieces of information are stored in the profile file. 

Very high end monitors, such as the HP Dreamcolor and various EIZOs  expose a LUT that can be used for color translation.  While the DDC protocol allows for manipulating the monitor LUT, some of these monitors instead use a special connection, such as USB, and a proprietary protocol.  Very high end LCD monitors will also have separate red, green, and blue LEDs or CCFL tubes, so that the color temperature can be precisely adjusted.  But most flat panel monitors  have just one one type of "white" LED or CCFL tube. 

Linux calibration programs, such as Argyllcms, simply take as given whatever state the on screen controls have put the monitor into, and build the lookup table for that state.   dispcalGUI has an optional pre-calibration step which displays the measured color temperature of the display and suggests which physical monitor controls the user should change to achieve the desired color temperature.

There's a school of thought that holds that unless you have a high end monitor, you're actually best off setting all the controls (excepting brightness) on a LCD monitor to the factory defaults.  Graeme Gill, create of Argyllcms, has a well considered discussion of this question (http://www.arbyllcms/monitor-controls.html).

>> As for the proprietary fglrx driver, it doesn't even create the
>> /dev/i2c-n devices.   End of story.
>
> I've seen that, yes, that's sad. Well AFAIK the open-source radeon
> driver is better and more popular than the nouveau driver so the impact
> is limited, but it would still be good if the fglrx driver would expose
> the i2c buses.

Well, it turns out that while fglrx doesn't expose /dev/i2c devices, it has a group of APIs for accessing the I2C bus, executing the DDC protocol, reading the EDID, etc.  notably.:
   ADL_Display_WriteAndReadI2C()
   ADL_Display_DDCBlockAccess_Get()
   ADL_Display_EdidData_Get()

After more munging, I'm able to use this API (called ADL) to communicate with monitors using DDC.   The code is actually simpler than the /dev/i2c version, because the API  knows something about the DDC protocol, handles retries, etc.  ( Interestingly, there's ifdef'd code in ddccontrol to use ADL.  Whether it actually works I don't know. )


>
>
> Or alternatively, the X11 stack could provide a DDC/CI API which
> drivers would implement and color profile tools would use. I personally
> don't care if this happens in X11 or through /dev/i2c* nodes, I'm not
> familiar enough with the area to tell which approach is the best.

From an overall architecture perspective, it seems to me that X11 is the desirable place from a DDC/CI API.   I'm also sure that there are excellent reasons, far beyond my pay grade, why this has not been implemented.   However, there does seem to be discussion of putting support into Wayland.   See, for example, this comment by Richard Hughes (author of colord) on the Wayland list: http://lists.freedesktop.org/archives/wayland-devel/2013-April/008406.html

>> 2) It's time to head over to a nvidia forum to explore why the code 
>> doesn't work with the proprietary driver.  Maybe there's a work around, 
>> or maybe I can light a fire under the nvidia developers.
>
> Good idea, I hope you get somewhere with that. Odds are that this part
> of the code saw little testing so far, because almost all device
> drivers use the SMBus command set rather than raw I2C messages.
>

I was able to narrow the nvidia driver problem to just the write() call, so the delay between write() and read() plays no role.  Moreover, I only see the problem on my (relatively) new GTX660Ti.   write() calls of 1 or 2 bytes succeed.   Calling write() for 3 or more bytes fails.   The problem does not occur on an older GT220 card.  So it appears that the driver was not properly upgraded to handle newer cards.   I posted a report along with sample code on the Nvidia developer website ( https://devtalk.nvidia.com/default/topic/760798/linux/ddc-ci-over-i2c-fails-gtx660ti/ ).  Other than a comment from one other user who believed that this was the same issue as the known problem of ddccontrol not working for more recent cards, there's been no response.

Lastly, a question.  I have one monitor, a relatively recent "professional" quality Dell P2411H, which has a propensity to return duplicated bytes on read(), .e.g.  0x01020203 instead of 0x010203.  The DDC protocol works, but has high retry counts.   Any thoughts on what I might do here as a workaround? 

Regards,
Sanford






